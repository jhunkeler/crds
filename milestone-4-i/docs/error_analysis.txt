These notes summarize progress and status for informal comparison
tests between CDBS and CRDS.  The work centers around explaining
discrepancies between CRDS and CDBS given dataset input parameters
taken from the HST catalog and best reference outputs taken from the
HST catalog and CRDS.  My intent is that the following test cases
comprise all known datasets for modern HST instruments.

In the tables below, the column "mismatch errors" shows errors
versus the total number of dataset comparisons.

Status reported in March:   

|| '''task'''         || '''mismatch errors''' || 
||    ACS mappings    ||       1909 / 59006    ||
||    COS mappings    ||       0  / 8877       ||
||    NICMOS mappings ||    2393 / 116078      ||
||    STIS mappings   ||     0  / 144711       ||
||    WFPC2 mappings  ||    9  / 186480        ||
||    WFC3 mappings   ||     0  / 34616        ||

Status now:

|| '''task'''         || '''mismatch errors''' ||
||    ACS mappings    ||     1488 / 41121      ||
||    COS mappings    ||     1038  / 4752      ||
||    NICMOS mappings ||     1121 / 116078     ||
||    STIS mappings   ||     0  / 146319       ||
||    WFPC2 mappings  ||     11  / 186480      ||
||    WFC3 mappings   ||     3745  / 37483     ||


Further categorizing mismatch errors:
-------------------------------------

ACS    mostly pfltfile, biasfile,  some darkfile 
	   Most pflt errors have partial signature (nar1136nj_pfl.fits vs. nar1136pj_pfl.fits) ~900 errors in 2 equivalence classes
	   deemed a likely CDBS error MAST correction no effect

COS    all errors are related to a CDBS change in the handling of xtractab
	   analysis indicates the catalog is now out of date.  MAST data to confusing to get w/o help.
     
NICMOS notable progress using MAST data
	   all errors in 2 equivalence classes for darkfile

WFC3   discovery of masked errors and missing special case code in CRDS
	   almost 3000 new darkfile errors in one equivalence class new today (whackamole!)

WFPC2  11 errors in 2 equivalence classes
	   haven't bothered

In general:  reductions in overall dataset counts were due to discovery 
   			 and removal of duplicate datasets.

Both ACS and WFC3 have special case code involved.

Difficulty with using MAST
--------------------------

1.  Requirement to minimize MAST dataset downloads necessitates
    defining dataset equivalence sets where any dataset in an ES is
    suitable for representing the CDBS parameters and answers of all
    members of the set.  This was complicated and is now a 3rd
    generation effort with each stage requiring extensive refactoring
    of the CRDS test code.  Even "done right" no two datasets are
    trivially equivalent because they have different exposure start
    times; so my best effort at defining equivalence still has a
    measure of doubt involved.

2.  Asking for datasets, there is ambiguity about which extension to ask
    for,  and ambiguity about where exactly the corrected CDBS answers
    are.   How does one get an updated dataset and be sure it is updated?

3.  Asking for MAST datasets,  they're not always delivered.   This
    may be a problem in my interpretation of the database and definition
    of "dataset".  Members of any associations are also included,  and 
    sometimes the reference files themselves are included;  both vastly
    expand the size of the original request without any confirmation.

4.  When datasets are successfully delivered, there's additional
    complexity correctly applying them as surrogates for others in the
    same equivalence set.

In summary,  working with the catalog and attempting to enhance it with MAST
datasets,  there are problems or complexity with:

- defining the unique datasets which should be tested/compared
  (duplicates and associations)

- defining equivalent dataset error classes (which erring datasets
  represent the same case?  did I define "equivalent" correctly?)

- obtaining large numbers of representative MAST datasets (It's hard
  to track all the failures.   Sometimes MAST is saying "no".)

- applying equivalent datasets as substitute answers (did I use the
  new MAST data when I should have?)

- knowing whether or not the correct data was requested from MAST
  (I asked MAST for *something*.  Does it actually have better answers?
  What types of files am I supposed to be asking for?  I want to ask
  for a "dataset" but I'm given a list of 5-6 obscure 3 character 
  file types to choose from.)

- afterward, knowing whether an error case now has a MAST correction

- getting corrections for all small equivalence classes may be too
  many files to ask for from MAST.  this is improving though if I could
  limit unintended downloads getting all might be acceptable.

- implementing equivalence classes required calling CRDS significantly
  differently since I needed to go reftype-by-reftype injecting extra
  code.

These are all sources of potential error and implementation work,
resulting in whackamole.   

We have these questions:

1. Did CRDS give the wrong answer?
2. Did CDBS give the wrong answer?

but due to the catalog/MAST approach these extra questions:

3. Did I get all the right datasets from the catalog?
4. Did I accurately characterize equivalent datasets?
5. Did I ask for an improved MAST answer?
6. Did I get the answer I asked for?
7. Did I correctly apply the answer I got?

At intervals,  the answers to 3-7 are "no".

Although I was initially optimistic about the MAST approach, it has
gotten so complicated that I'll be less confident about the results
even if it eventually appears to work.  As a remedy, I think we need
access to the OPUS environment after all, the sooner, the better.
The CDBS team (namely Mike) has been supportive in offering the
possibility of running closer to OPUS.   I was reluctant to do that,
because it's harder in some ways,  and also because it's hard to
admit that the catalog/MAST approach isn't working.

What we need in the simplest possible fashion is:

1. An accurate stream of dataset ids.  What *are* the datasets?  No
duplicates.  No associations unless those are required.

2. For each dataset,  what are the best ref parameters?   Alternately,
give us the .fits file somewhere.

3. For each dataset, what are the CDBS best reference recommendations?

In order to debug the problems in either CRDS or CDBS,  we need 1-3
in a reliable, bullet-proof, plug-in form.   Otherwise there are just
huge quantities of noise and development effort getting 1-3.   Note
that the entire catalog/MAST approach is just one way of getting 1-3,
and too complicated to work.



