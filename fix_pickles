#! /usr/bin/env python
#-*-python-*-

"""This script updates the database header pickles using calibrated datasets
downloaded from MAST (http://archive.stsci.edu/cgi-bin/dataset_lookup).  The
reason some datasets are downloaded is that OPUS and the catalog have 
different results sometimes but the OPUS/MAST OTFR is considered definitive.

The general approach for CRDS testing is as follows:

1. Dump dataset headers from the catalog DadsOps on zeppo.stsci.edu.  Create
header pickles consisting of critical best ref parameters and CDBS results.

2. Run CRDS using the header best ref parameters.   Compare the CRDS results
to the CDBS results.   Report and aggregate any mismatches based on the
(CDBS_answer, CRDS_answer) tuple,  an approximation to keying off header input
tuples.   Report only the first dataset of each wrong answer tuple.

3. For each wrong answer dataset reported,  download the dataset from MAST.

4. Update the CDBS/dataset header pickles using actual datasets from MAST 
rather than the catalog.

5. Re-run the comparison test to see which problems are resolved as an error
in the catalog and which remain.

IMPORTANT:  This code makes the assumption that all datasets associated with the
same wrong answer will produce the same right answer when run through the OTFR
used by MAST.   This is an assumption because there's no guarantee that all the
wrong answer file pairs correspond to the same input parameters,  hence CDBS 
might produce a different answer if the inputs corresponding to some dataset
actually do differ from the MAST dataset.   This code will make that case 
appear to work when it does not.   

This approximation could be fixed by reorganizing the mismatched lists to work 
off the hash of the input parameters tuple... but that is a fairly extensive 
change.
"""
import sys
import pyfits
import cPickle
import os.path

from crds import (log, utils)

import get_mast_dataset_ids

def get_fix_lists(errfile):
    """Pull all the dataset error lists out of errfile as a list of lists"""
    fix_lists = []
    for mismatched in get_mast_dataset_ids.get_mismatched_objects():
        fix_lists.append(mismatched.datasets)
    return fix_lists

def get_fix_list(dataset, fix_lists):
    """Return the list of datasets to which dataset is PROBABLY applicable.
    """
    for fix_list in fix_lists:
        if dataset in fix_list:
            return fix_list
    # Note:  this will happen when pickles have already been patched.
    log.warning("No fix list found for dataset", dataset)
    return []

def organize_datasets(datasets):
    organized = {}
    for dataset in datasets:
        log.info("Loading", repr(dataset))
        header = pyfits.getheader(dataset)
        try:
            instr = header["INSTRUME"].lower()
        except:
            log.error("Missing INSTRUME keyword in dataset", repr(dataset))
        if instr not in organized:
            organized[instr] = []
        dataset_id = os.path.basename(dataset).split("_")[0].upper()
        organized[instr].append((dataset_id, header))
    return organized

def fix_iraf_path(val):
    if not isinstance(val, str):
        return val
    if "ref$" in val.lower():
        val = val.split("$")[1].upper()
    return val

def fix_pickles(organized, fix_lists):
    """Apply the `organized` headers to each of the datasets in `fix_lists`.

    A dataset for which MAST answers exists will be applied to all datasets
    in the same fix_list to fix the catalog pickle.   If this eliminates the
    error for one dataset,  it should fix the entire list...   but this is still
    only an approximation.
    """
    for instr in organized:
        pickle = "../../../datasets/" + instr + "_headers.pkl"
        log.info("Loading pickle", repr(pickle))
        pheaders = cPickle.load(open(pickle))
        for dataset, header in organized[instr]:
            log.info("Processing", repr(dataset), eol="")
            fix_list = get_fix_list(dataset, fix_lists)
            for pheader in pheaders:
                p_dataset = pheader["DATA_SET"].upper()
                if p_dataset not in fix_list:
                    continue
                for key in pheader:
                    if key in ["DATA_SET"]:
                        continue
                    if key not in header:
                        log.warning("Missing key in dataset", repr(dataset),
                                    "=", repr(key))
                        continue
                    hval = utils.condition_value(fix_iraf_path(header[key]))
                    pval = utils.condition_value(pheader[key])
                    if hval != pval:
                        log.info("Updating dataset", repr(p_dataset),
                                 "key", repr(key),
                                 "from", pval, "to", hval)
                        pheader[key] = hval
        log.info("Saving pickle", repr(pickle))
        file = open(pickle, "w+")
        cPickle.dump(pheaders, file)
        file.close()

def main(errfile, datasets):
    organized = organize_datasets(datasets)
    fix_lists = get_fix_lists(errfile)
    fix_pickles(organized, fix_lists)
    log.write()
    log.standard_status()

if __name__  == "__main__":
    main(errfile = sys.argv[1], datasets = sys.argv[2:])
